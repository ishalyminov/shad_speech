{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture #11 Seminar \"WaveNet inference\" üåä\n",
    "\n",
    "\n",
    "The purpose of this seminar is to immerse you in WaveNet architecture. In the lecture we discussed in detail WaveNet architecture, so you can easily write an autoregressive inference function.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Mark-Hasegawa-Johnson/publication/311106829/figure/fig3/AS:433958858039299@1480475262377/The-caching-scheme-for-efficient-generation-Due-to-dilated-convolutions-the-size-of-the.png\" alt=\"Drawing\" style=\"width: 60%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.io.wavfile import read\n",
    "from scipy.signal import lfilter\n",
    "from queue import Queue\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSP utils \n",
    "\n",
    "\"D\" in \"Deep Learning\" stands for data and \"e\" for \"engineering\" üôÉ, so we need to implement some basic data preprocessing functions. Specifically we need to implement proper conversion from signal to mel spectrogram.\n",
    "\n",
    "All preprocessing functions are already implemented, but we don't want you to pass them by üòè. So you need to implement their reverse counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    MAX_WAV_VALUE = 32768.0\n",
    "\n",
    "    def __init__(self):\n",
    "        num_frequencies = 1025\n",
    "        self.sample_rate = 24000\n",
    "        self.window_size = int(self.sample_rate * 0.05)\n",
    "        self.window_step = self.window_size // 4\n",
    "        self.n_fft = (num_frequencies - 1) * 2\n",
    "        self.preemphasis_coef = 0.97\n",
    "        self.min_frequency = 50\n",
    "        self.max_frequency = 12000\n",
    "        self.num_mel_bins = 80\n",
    "        self.ref_level_db = 20\n",
    "        self.min_level_db = -100\n",
    "        \n",
    "        self.min_level = np.exp(self.min_level_db / 20 * np.log(10))\n",
    "\n",
    "        self.mel_basis = librosa.filters.mel(\n",
    "            self.sample_rate,\n",
    "            n_fft=self.n_fft,\n",
    "            n_mels=self.num_mel_bins,\n",
    "            fmin=self.min_frequency,\n",
    "            fmax=self.max_frequency)\n",
    "        self.inv_mel_basis = np.linalg.pinv(self.mel_basis)\n",
    "        \n",
    "    def load_wav(self, path):\n",
    "        sr, signal = read(path)\n",
    "        if signal.dtype == np.int16:\n",
    "            signal = signal.astype(np.float32) / self.MAX_WAV_VALUE\n",
    "        assert sr == self.sample_rate\n",
    "        return signal\n",
    "\n",
    "    def stft(self, y):\n",
    "        return librosa.stft(y,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.window_step,\n",
    "            win_length=self.window_size)\n",
    "\n",
    "    def istft(self, y):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        return librosa.istft(y,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.window_step,\n",
    "            win_length=self.window_size)\n",
    "        pass\n",
    "\n",
    "    def pre_emphasis(self, x):\n",
    "        return lfilter([1, -self.preemphasis_coef], [1], x)\n",
    "\n",
    "    def de_emphasis(self, x):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def amp_to_db(self, x):\n",
    "        x = np.maximum(self.min_level, x)\n",
    "        return 20 * np.log10(x) - self.ref_level_db\n",
    "\n",
    "    def db_to_amp(self, x):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def normalize(self, S):\n",
    "        return np.clip((S - self.min_level_db) / -self.min_level_db, 0, 1)\n",
    "\n",
    "    def inv_normalize(self, S):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def linear_to_mel(self, S):\n",
    "        return np.dot(self.mel_basis, S)\n",
    "\n",
    "    def mel_to_linear(self, M):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def spectrogram(self, y):\n",
    "        S = np.abs(self.stft(y))\n",
    "        S = self.amp_to_db(S)\n",
    "        return self.normalize(S)\n",
    "\n",
    "    def inv_spectrogram(self, S):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "    \n",
    "    def mel_spectrogram(self, y):\n",
    "        S = np.abs(self.stft(y))\n",
    "        M = self.linear_to_mel(S)\n",
    "        M = self.amp_to_db(M)\n",
    "        return self.normalize(M)\n",
    "\n",
    "    def inv_mel_spectrogram(self, M):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def griffin_lim(self, x, num_iter=20):\n",
    "        # not the first time, I know :)\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def mu_law_encode(self, x, mu=256):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "\n",
    "    def mu_law_decode(self, x, mu=256):\n",
    "        #####\n",
    "        # ...\n",
    "        #####\n",
    "        pass\n",
    "    \n",
    "ap = AudioProcessor()\n",
    "\n",
    "x = ap.load_wav('./samples/00000.wav')\n",
    "spec = ap.spectrogram(ap.pre_emphasis(x))\n",
    "mel = ap.mel_spectrogram(ap.pre_emphasis(x))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(spec, aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 2))\n",
    "plt.imshow(mel, aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function lfilter in module scipy.signal.signaltools:\n",
      "\n",
      "lfilter(b, a, x, axis=-1, zi=None)\n",
      "    Filter data along one-dimension with an IIR or FIR filter.\n",
      "    \n",
      "    Filter a data sequence, `x`, using a digital filter.  This works for many\n",
      "    fundamental data types (including Object type).  The filter is a direct\n",
      "    form II transposed implementation of the standard difference equation\n",
      "    (see Notes).\n",
      "    \n",
      "    The function `sosfilt` (and filter design using ``output='sos'``) should be\n",
      "    preferred over `lfilter` for most filtering tasks, as second-order sections\n",
      "    have fewer numerical problems.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    b : array_like\n",
      "        The numerator coefficient vector in a 1-D sequence.\n",
      "    a : array_like\n",
      "        The denominator coefficient vector in a 1-D sequence.  If ``a[0]``\n",
      "        is not 1, then both `a` and `b` are normalized by ``a[0]``.\n",
      "    x : array_like\n",
      "        An N-dimensional input array.\n",
      "    axis : int, optional\n",
      "        The axis of the input data array along which to apply the\n",
      "        linear filter. The filter is applied to each subarray along\n",
      "        this axis.  Default is -1.\n",
      "    zi : array_like, optional\n",
      "        Initial conditions for the filter delays.  It is a vector\n",
      "        (or array of vectors for an N-dimensional input) of length\n",
      "        ``max(len(a), len(b)) - 1``.  If `zi` is None or is not given then\n",
      "        initial rest is assumed.  See `lfiltic` for more information.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    y : array\n",
      "        The output of the digital filter.\n",
      "    zf : array, optional\n",
      "        If `zi` is None, this is not returned, otherwise, `zf` holds the\n",
      "        final filter delay values.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    lfiltic : Construct initial conditions for `lfilter`.\n",
      "    lfilter_zi : Compute initial state (steady state of step response) for\n",
      "                 `lfilter`.\n",
      "    filtfilt : A forward-backward filter, to obtain a filter with linear phase.\n",
      "    savgol_filter : A Savitzky-Golay filter.\n",
      "    sosfilt: Filter data using cascaded second-order sections.\n",
      "    sosfiltfilt: A forward-backward filter using second-order sections.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The filter function is implemented as a direct II transposed structure.\n",
      "    This means that the filter implements::\n",
      "    \n",
      "       a[0]*y[n] = b[0]*x[n] + b[1]*x[n-1] + ... + b[M]*x[n-M]\n",
      "                             - a[1]*y[n-1] - ... - a[N]*y[n-N]\n",
      "    \n",
      "    where `M` is the degree of the numerator, `N` is the degree of the\n",
      "    denominator, and `n` is the sample number.  It is implemented using\n",
      "    the following difference equations (assuming M = N)::\n",
      "    \n",
      "         a[0]*y[n] = b[0] * x[n]               + d[0][n-1]\n",
      "           d[0][n] = b[1] * x[n] - a[1] * y[n] + d[1][n-1]\n",
      "           d[1][n] = b[2] * x[n] - a[2] * y[n] + d[2][n-1]\n",
      "         ...\n",
      "         d[N-2][n] = b[N-1]*x[n] - a[N-1]*y[n] + d[N-1][n-1]\n",
      "         d[N-1][n] = b[N] * x[n] - a[N] * y[n]\n",
      "    \n",
      "    where `d` are the state variables.\n",
      "    \n",
      "    The rational transfer function describing this filter in the\n",
      "    z-transform domain is::\n",
      "    \n",
      "                             -1              -M\n",
      "                 b[0] + b[1]z  + ... + b[M] z\n",
      "         Y(z) = -------------------------------- X(z)\n",
      "                             -1              -N\n",
      "                 a[0] + a[1]z  + ... + a[N] z\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Generate a noisy signal to be filtered:\n",
      "    \n",
      "    >>> from scipy import signal\n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> t = np.linspace(-1, 1, 201)\n",
      "    >>> x = (np.sin(2*np.pi*0.75*t*(1-t) + 2.1) +\n",
      "    ...      0.1*np.sin(2*np.pi*1.25*t + 1) +\n",
      "    ...      0.18*np.cos(2*np.pi*3.85*t))\n",
      "    >>> xn = x + np.random.randn(len(t)) * 0.08\n",
      "    \n",
      "    Create an order 3 lowpass butterworth filter:\n",
      "    \n",
      "    >>> b, a = signal.butter(3, 0.05)\n",
      "    \n",
      "    Apply the filter to xn.  Use lfilter_zi to choose the initial condition of\n",
      "    the filter:\n",
      "    \n",
      "    >>> zi = signal.lfilter_zi(b, a)\n",
      "    >>> z, _ = signal.lfilter(b, a, xn, zi=zi*xn[0])\n",
      "    \n",
      "    Apply the filter again, to have a result filtered at an order the same as\n",
      "    filtfilt:\n",
      "    \n",
      "    >>> z2, _ = signal.lfilter(b, a, z, zi=zi*z[0])\n",
      "    \n",
      "    Use filtfilt to apply the filter:\n",
      "    \n",
      "    >>> y = signal.filtfilt(b, a, xn)\n",
      "    \n",
      "    Plot the original signal and the various filtered versions:\n",
      "    \n",
      "    >>> plt.figure\n",
      "    >>> plt.plot(t, xn, 'b', alpha=0.75)\n",
      "    >>> plt.plot(t, z, 'r--', t, z2, 'r', t, y, 'k')\n",
      "    >>> plt.legend(('noisy signal', 'lfilter, once', 'lfilter, twice',\n",
      "    ...             'filtfilt'), loc='best')\n",
      "    >>> plt.grid(True)\n",
      "    >>> plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lfilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all tests are passed\n",
    "\n",
    "x = ap.load_wav('./samples/00000.wav')\n",
    "\n",
    "assert np.abs(ap.de_emphasis(ap.pre_emphasis(x)) - x).max() < 1e-10\n",
    "\n",
    "assert ap.mu_law_encode(-1.0) == 0\n",
    "assert ap.mu_law_encode(0.0) == 128\n",
    "assert ap.mu_law_encode(0.5) == 239\n",
    "assert ap.mu_law_encode(1.0) == 255\n",
    "for i in range(256):\n",
    "    assert ap.mu_law_encode(ap.mu_law_decode(i)) == i\n",
    "\n",
    "# make sure the quality does not degrade too much\n",
    "display(Audio(x, rate=ap.sample_rate))\n",
    "display(Audio(ap.inv_spectrogram(ap.spectrogram(x)), rate=ap.sample_rate))\n",
    "display(Audio(ap.inv_mel_spectrogram(ap.mel_spectrogram(x)), rate=ap.sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Griffin-Lim with the synthesised üåÆ mels\n",
    "\n",
    "Hear how the result of Griffin-Lim sounds, not just on mel spectrograms, but on **generated** mel spectrograms. But it was generated using teacher forcing, otherwise it will not be possible to accurately correlate the frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ap.load_wav('./samples/00000.wav')\n",
    "m = np.load('./samples/00000.npy')\n",
    "\n",
    "plt.figure(figsize=(12, 2))\n",
    "plt.imshow(m.T, aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "display(Audio(x, rate=ap.sample_rate))\n",
    "display(Audio(ap.de_emphasis(ap.inv_mel_spectrogram(m.T)), rate=ap.sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveNet\n",
    "\n",
    "Read the code carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CausalConv1d, self).__init__(*args, **kwargs)\n",
    "        self.padding = ((self.kernel_size[0] - 1) * self.dilation[0],)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super(CausalConv1d, self).forward(x)\n",
    "        return x[:, :, :-self.padding[0]]\n",
    "\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    \"\"\"WaveNet architecture with local conditioning\n",
    "\n",
    "    https://arxiv.org/pdf/1609.03499.pdf - original paper\n",
    "    https://arxiv.org/pdf/1702.07825.pdf - appending A for more details\n",
    "    \n",
    "    But given implementation has following differences:\n",
    "    1. tanh is not applied to input embedding\n",
    "    2. vector is scaled (multiplied 0.5 ** 0.5) between blocks\n",
    "    3. GRU is used for processing mel spectrogram\n",
    "    4. GRU output is nearest neighbour apsampled hop_size times\n",
    "    5. each block has own conditioning projection\n",
    "\n",
    "    Args:\n",
    "        num_channels       (int): size of modelled categorical distribution\n",
    "        residual_channels  (int): hidden vector size\n",
    "        gate_channels      (int): gate block dimension\n",
    "        skip_channels      (int): skip-vector size\n",
    "        pre_channels       (int): dimension before the last layer\n",
    "        dilation_cycles    (int): number of dilation cycles\n",
    "        dilation_depth     (int): blocks number in dilation cycle\n",
    "        condition_channels (int): number of mel filters\n",
    "        hop_size           (int): STFT hop size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_channels,\n",
    "                 residual_channels,\n",
    "                 gate_channels,\n",
    "                 skip_channels,\n",
    "                 pre_channels,\n",
    "                 dilation_cycles,\n",
    "                 dilation_depth,\n",
    "                 condition_channels,\n",
    "                 hop_size):\n",
    "        super(WaveNet, self).__init__()\n",
    "        \n",
    "        self.kernel_size = 2\n",
    "        self.dilations = np.array([\n",
    "            2 ** (i % dilation_depth) \n",
    "            for i in range(dilation_cycles * dilation_depth)\n",
    "        ])\n",
    "        \n",
    "        self.num_channels = num_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.gate_channels = gate_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.pre_channels = pre_channels\n",
    "        self.hop_size = hop_size\n",
    "        \n",
    "        self.condition_net = nn.GRU(\n",
    "            input_size=condition_channels,\n",
    "            hidden_size=condition_channels // 2,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "        \n",
    "        self.conv_input = nn.Conv1d(\n",
    "            in_channels=num_channels,\n",
    "            out_channels=residual_channels,\n",
    "            kernel_size=1)\n",
    "\n",
    "        self.blocks_conv_filter = nn.ModuleList([\n",
    "            CausalConv1d(\n",
    "                in_channels=residual_channels,\n",
    "                out_channels=gate_channels,\n",
    "                kernel_size=2,\n",
    "                dilation=d\n",
    "            ) for d in self.dilations])\n",
    "\n",
    "        self.blocks_conv_gate = nn.ModuleList([\n",
    "            CausalConv1d(\n",
    "                in_channels=residual_channels,\n",
    "                out_channels=gate_channels,\n",
    "                kernel_size=2,\n",
    "                dilation=d\n",
    "            ) for d in self.dilations])\n",
    "        \n",
    "        self.blocks_conv_residual = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=gate_channels,\n",
    "                out_channels=residual_channels,\n",
    "                kernel_size=1\n",
    "            ) for _ in range(len(self.dilations) - 1)])\n",
    "        \n",
    "        self.blocks_conv_skip = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=gate_channels,\n",
    "                out_channels=skip_channels,\n",
    "                kernel_size=1\n",
    "            ) for _ in range(len(self.dilations))])\n",
    "        \n",
    "        self.blocks_conv_cond = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=condition_channels,\n",
    "                out_channels=gate_channels * 2,\n",
    "                kernel_size=1\n",
    "            ) for _ in range(len(self.dilations))])\n",
    "        \n",
    "        self.conv_out_1 = nn.Conv1d(\n",
    "            in_channels=skip_channels,\n",
    "            out_channels=pre_channels,\n",
    "            kernel_size=1)\n",
    "        self.conv_out_2 = nn.Conv1d(\n",
    "            in_channels=pre_channels,\n",
    "            out_channels=num_channels,\n",
    "            kernel_size=1)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (FloatTensor): continuous audio signal [B x K x T]\n",
    "            c (FloatTensor): local condition features [B x L x C],\n",
    "                where L = T // 300\n",
    "\n",
    "        Returns:\n",
    "            FloatTensor: output [B x out_channels x T]\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.conv_input(x)\n",
    "        c, _ = self.condition_net(c)\n",
    "        c = c.transpose(1, 2)\n",
    "\n",
    "        c = nn.functional.interpolate(c,\n",
    "            scale_factor=self.hop_size,\n",
    "            mode='nearest')\n",
    "\n",
    "        assert c.size(2) == x.size(2)\n",
    "\n",
    "        x_acc = 0\n",
    "        for b in range(len(self.dilations)):\n",
    "            x_filter = self.blocks_conv_filter[b](x)\n",
    "            x_gate = self.blocks_conv_gate[b](x)\n",
    "\n",
    "            cond = self.blocks_conv_cond[b](c)\n",
    "            c_filter, c_gate = cond.chunk(chunks=2, dim=1)\n",
    "            x_filter += c_filter\n",
    "            x_gate += c_gate\n",
    "\n",
    "            x_hidden = torch.tanh(x_filter) * torch.sigmoid(x_gate)\n",
    "\n",
    "            x_skip = self.blocks_conv_skip[b](x_hidden)\n",
    "            x_acc = x_acc + x_skip\n",
    "\n",
    "            if b < len(self.dilations) - 1:\n",
    "                x_residual = self.blocks_conv_residual[b](x_hidden)\n",
    "                x = x + x_residual\n",
    "\n",
    "            x = x * 0.5 ** 0.5\n",
    "            \n",
    "\n",
    "        x = self.conv_out_1(torch.relu(x_acc))\n",
    "        x = self.conv_out_2(torch.relu(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaveNet(\n",
    "    num_channels=256,\n",
    "    residual_channels=64,\n",
    "    gate_channels=64,\n",
    "    skip_channels=128,\n",
    "    pre_channels=256,\n",
    "    dilation_cycles=4,\n",
    "    dilation_depth=10,\n",
    "    condition_channels=80,\n",
    "    hop_size=300)\n",
    "model.load_state_dict(torch.load('./state_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, num_channels):\n",
    "    x_o = torch.FloatTensor(x.shape[0], num_channels, x.shape[1])\n",
    "    x_o.zero_().scatter_(1, x.unsqueeze(1), 1)\n",
    "    return x_o\n",
    "\n",
    "def calc_loss(model, x, c):\n",
    "    x_o = one_hot(x, model.num_channels)\n",
    "    y = model.forward(x_o, c).transpose(1, 2)\n",
    "\n",
    "    loss = nn.functional.cross_entropy(\n",
    "        y[:, :-1].contiguous().view(-1, y.shape[-1]),\n",
    "        x[:, 1:].contiguous().view(-1))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = ap.load_wav('./samples/00000.wav')\n",
    "    c = np.load('./samples/00000.npy')\n",
    "    \n",
    "    # cut off to be a multiple of the window step\n",
    "    c = c[:len(x) // ap.window_step]\n",
    "    x = x[:len(c) * ap.window_step]\n",
    "\n",
    "    # apply mu-law encoding\n",
    "    x = ap.mu_law_encode(x)\n",
    "\n",
    "    x = torch.LongTensor(x)\n",
    "    c = torch.FloatTensor(c)\n",
    "\n",
    "    loss = calc_loss(model, x.unsqueeze(0), c.unsqueeze(0)).item()\n",
    "\n",
    "assert np.allclose(loss, 1.7863293886184692)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveNet Inference \n",
    "\n",
    "You need to implement the `infer` function, which synthesizes the audio from the `mel` spectrogram by the `model` model. The output of this function is mu-law encoded signal.\n",
    "\n",
    "**Important note**: as you can see from the code, we calculate spectrograms passing them through the pre-emphasis filter. It so happened (obviously by mistake) that although WaveNet learned with such spectrograms, but the audio signal for was not passed through the pre-emphasis filter. So you **do not need** to pass WaveNet output through de-emphassis filter.\n",
    "\n",
    "Hints:\n",
    "1. debug on short spectra (30-40 frames long)\n",
    "2. parse network to get matrices and vectors -- it's easier to work directly with them\n",
    "3. sanity check matrices sizes that they have expected shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, mel):\n",
    "    #####\n",
    "    # ...\n",
    "    #####\n",
    "    return np.array([0])\n",
    "    \n",
    "\n",
    "\n",
    "x = ap.load_wav('./samples/00000.wav')\n",
    "x_gen = ap.mu_law_decode(infer(model, np.load('./samples/00000.npy')))\n",
    "\n",
    "display(Audio(x, rate=ap.sample_rate))\n",
    "display(Audio(x_gen, rate=ap.sample_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
