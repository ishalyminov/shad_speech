{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a01d627",
   "metadata": {},
   "source": [
    "# FastPitch + GST\n",
    "At this assignment you should implement and train FastPitch AM with GST encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa992050",
   "metadata": {},
   "source": [
    "# Assessment\n",
    "\n",
    "Total score for this task is 30\n",
    "\n",
    "Categorically:\n",
    "* **3** - you filled in all the code gaps before the GST estimator block (and they seem sane to the reviewer)\n",
    "* **9** - you trained fp model\n",
    "* **3** - you visualized the GST space\n",
    "* **bonus 5** - you found some interesting (speed, speech style etc.) cluster in the corpus and tried to re-synthesize one utterance with a style of another \n",
    "* **3** - you filled in all the code gaps after the GST estimator block (and they seem sane to the reviewer)\n",
    "* **6** - you trained a GST estimator on the top of your FP model\n",
    "* **3** - your test files sound intelligible and corresponds to the text\n",
    "* **3** - your test files have normal or good intonation and sound natural\n",
    "\n",
    "The result of this work is the notebook filled and 16 wavfiles (15 test corpus + one about a quick brown fox).\n",
    "Please pack them in archive with the name {your last name}_{your first name}_hw3_fastpitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba713d74",
   "metadata": {},
   "source": [
    "## Dependencies & Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6085f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install cudatoolkit==11.1 pytorch==1.8.1 # -- should already be at DataSphere\n",
    "# !pip install librosa praat-parselmouth torch-optimizer tensorboardX Unidecode inflect wget tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2530eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5607c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import re\n",
    "import wget\n",
    "import time\n",
    "import shutil\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch_optimizer import Lamb\n",
    "from torch.optim import Adam\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import parselmouth\n",
    "from scipy.io import wavfile\n",
    "import IPython.display as ipd\n",
    "\n",
    "from hparams import HParamsFastpitch\n",
    "from nv_extern.tacotron2.arg_parser import parse_tacotron2_args\n",
    "from nv_extern.tacotron2.model import Tacotron2\n",
    "from nv_extern.waveglow.model import WaveGlow\n",
    "from nv_extern.waveglow.denoiser import Denoiser\n",
    "from nv_extern.common.text.symbols import get_symbols\n",
    "from nv_extern.common.text.text_processing import TextProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb14588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available(), \"Need some GPU to train the model\"\n",
    "device = torch.device('cuda')\n",
    "print(\"GPU found: {}\".format(torch.cuda.get_device_name(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b95925",
   "metadata": {},
   "outputs": [],
   "source": [
    "ljspeech_data_path = \"LJSpeech-1.1\"\n",
    "prepr_data_path = \"LJSpeech_prepr\"\n",
    "\n",
    "train_filelist_path = \"filelists/train_filelist.txt\"\n",
    "val_filelist_path = \"filelists/val_filelist.txt\"\n",
    "\n",
    "\n",
    "logs_dir = \"logs_dir\"\n",
    "gst_logs_dir = \"gst_logs_dir\"\n",
    "\n",
    "pretrained_models_path = \"pretrained_models\"\n",
    "\n",
    "if not os.path.exists(pretrained_models_path):\n",
    "    os.mkdir(pretrained_models_path)\n",
    "    \n",
    "synth_wavs_path = \"synth_wavs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0eaefde",
   "metadata": {},
   "outputs": [],
   "source": [
    "_prev_bytes = 0\n",
    "def bar_progress(current, total, width=80):\n",
    "    global _prev_bytes\n",
    "    \n",
    "    pq = _prev_bytes / total\n",
    "    cq = current / total\n",
    "    if _prev_bytes > current or (cq - pq) * 1000 > 1:\n",
    "        _prev_bytes = current\n",
    "    else:\n",
    "        return\n",
    "    progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (cq * 100, current, total)\n",
    "    # Don't use print() as it will print in new line every time.\n",
    "    sys.stdout.write(\"\\r\" + progress_message)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def getfile(url, dest):\n",
    "    wget.download(url, dest, bar=bar_progress)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f10da98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters redefinitions:\n",
      "batch_size = 8\n"
     ]
    }
   ],
   "source": [
    "hparams = HParamsFastpitch({'batch_size' : 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c058a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_checkpoint(model, state_dict):\n",
    "    if not hasattr(model, 'module'):\n",
    "        state_dict = {\n",
    "            name.replace('module.', ''): param\n",
    "            for name, param in state_dict.items()\n",
    "        }\n",
    "    else:\n",
    "        for key, value in state_dict.items():\n",
    "            break\n",
    "        if key[:6] != 'module':\n",
    "            state_dict = {\n",
    "                'module.' + name :  param\n",
    "                for name, param in state_dict.items()\n",
    "            }\n",
    "            \n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11329075",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7557357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ljs_arch=\"LJSpeech-1.1.tar.bz2\"\n",
    "taco2_url = \"http://data.keithito.com/data/speech/\" + ljs_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc5d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ljs_arch):\n",
    "    getfile(taco2_url, ljs_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44530761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -xf {ljs_arch}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b53f4",
   "metadata": {},
   "source": [
    "## Mel, duration and pitch extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d11cbb",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25b0b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(prepr_data_path):\n",
    "    os.mkdir(prepr_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbbf9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filelist(fname):\n",
    "    data = []\n",
    "    with open(fname) as ifile:\n",
    "        for l in ifile:\n",
    "            fid, _, text = l.strip().split(\"|\")\n",
    "            data.append((fid, text))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bd6cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = parse_filelist(os.path.join(ljspeech_data_path, \"metadata.csv\"))\n",
    "\n",
    "tp = TextProcessing('english_basic',['english_cleaners'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bee46aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13100/13100 [00:13<00:00, 961.52it/s]\n"
     ]
    }
   ],
   "source": [
    "for file_id, text in tqdm.tqdm(fl):\n",
    "    text_encoded = np.array(tp.encode_text(text))\n",
    "    text_path = os.path.join(prepr_data_path, file_id + \".text.npy\")\n",
    "    np.save(text_path, text_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fac1c3",
   "metadata": {},
   "source": [
    "### mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c986b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_basis = librosa.filters.mel(hparams.sample_rate, \n",
    "                                hparams.n_fft, \n",
    "                                n_mels=hparams.n_mel_channels,\n",
    "                                fmin=hparams.min_frequency, \n",
    "                                fmax=hparams.max_frequency)\n",
    "\n",
    "\n",
    "def mel_spectrogram(audio, hparams):\n",
    "    def linear_to_mel(s):\n",
    "        return np.dot(mel_basis, s)\n",
    "    \n",
    "    def amp_to_db(x):\n",
    "        return np.log(np.maximum(1e-5, x))\n",
    "    \n",
    "    spectr = librosa.stft(audio, \n",
    "                            n_fft=hparams.n_fft, \n",
    "                            hop_length=hparams.hop_length,\n",
    "                            win_length=hparams.win_length,\n",
    "                            window=hparams.window)\n",
    "    \n",
    "    spectr = np.abs(spectr)\n",
    "    mel_spectr = amp_to_db(linear_to_mel(spectr))\n",
    "    return mel_spectr\n",
    "\n",
    "\n",
    "def get_mel(pcm_path, mel_path, hparams):\n",
    "    audio, sr = sf.read(pcm_path, dtype='float32')\n",
    "    audio = librosa.resample(audio, sr, hparams.sample_rate)\n",
    "    np.save(mel_path, mel_spectrogram(audio, hparams))\n",
    "\n",
    "\n",
    "def extract_mels(wav_path, out_dir, hparams):\n",
    "    for filename in tqdm.tqdm(os.listdir(wav_path)):\n",
    "        file_id = re.search(r\"(.*).wav\", filename).group(1)\n",
    "        mel_path = os.path.join(out_dir, file_id + \".mel\")\n",
    "        get_mel(os.path.join(wav_path, filename), mel_path, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68359349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13100/13100 [01:22<00:00, 159.36it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_mels(os.path.join(ljspeech_data_path, \"wavs\"), prepr_data_path, hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adcbcfb",
   "metadata": {},
   "source": [
    "### durations extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6b78e",
   "metadata": {},
   "source": [
    "As it was said on the lecture, we can extract durations with tacotron model.\n",
    "\n",
    "Firstly, we need to download one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bc561a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "taco2_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/tacotron2_pyt_ckpt_amp/versions/19.12.0/files/nvidia_tacotron2pyt_fp16.pt\"\n",
    "taco2_path = os.path.join(pretrained_models_path, \"tacotron2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e0abd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(taco2_path):\n",
    "    getfile(taco2_url, taco2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e6085a",
   "metadata": {},
   "source": [
    "now we should extract durations by batch-processing our records with tacotron,\n",
    "\n",
    "then, we should store them into npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e89b7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TacoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, directory):\n",
    "        super(TacoDataset, self).__init__()\n",
    "        self.directory = directory\n",
    "        self.mel_paths = []\n",
    "        self.text_paths = []\n",
    "        self.file_ids = []\n",
    "        \n",
    "        for filename in os.listdir(directory):\n",
    "            if \".text.npy\" not in filename:\n",
    "                continue\n",
    "            file_id = re.search(r\"(.*).text.npy\", filename).group(1)\n",
    "            self.file_ids.append(file_id)\n",
    "            text_path = os.path.join(self.directory, filename)\n",
    "            self.text_paths.append(text_path)\n",
    "            self.mel_paths.append(text_path.replace(\".text.npy\", \".mel.npy\"))\n",
    "            \n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.IntTensor(np.load(self.text_paths[idx]))\n",
    "        mel = torch.FloatTensor(np.load(self.mel_paths[idx]))\n",
    "        file_id = self.file_ids[idx]\n",
    "        return (text, mel, file_id)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_paths)\n",
    "\n",
    "    \n",
    "class TacoCollate:\n",
    "    def __init__(self, n_mel_channels):\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "        \n",
    "    def  __call__(self, batch):\n",
    "        batch = sorted(batch, key=lambda x: x[0].size(0), reverse=True)\n",
    "        \n",
    "        max_text_length = max([text.size(0) for text, *_ in batch])\n",
    "        max_mel_length = max([mel.size(1) for _, mel, *_ in batch])\n",
    "        \n",
    "        texts_padded = torch.LongTensor(len(batch), max_text_length).zero_()\n",
    "        text_lengths = torch.LongTensor(len(batch)).zero_()\n",
    "        mels_padded = torch.FloatTensor(len(batch), self.n_mel_channels, max_mel_length).zero_()\n",
    "        mel_lengths = torch.LongTensor(len(batch)).zero_()\n",
    "        \n",
    "        file_ids = []\n",
    "        \n",
    "        for i, (text, mel, file_id) in enumerate(batch):\n",
    "            texts_padded[i, :text.size(0)] = text\n",
    "            text_lengths[i] = text.size(0)\n",
    "            mels_padded[i, :, :mel.size(1)] = mel\n",
    "            mel_lengths[i] = mel.size(1)\n",
    "            file_ids.append(file_id)\n",
    "            \n",
    "        max_len = torch.max(text_lengths.data).item()\n",
    "        return (texts_padded, text_lengths, mels_padded, max_len, mel_lengths), file_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ddc8e",
   "metadata": {},
   "source": [
    "As it was said, you can get good alignment by dynamic programming.\n",
    "Here, you should write some code that will find the trajectory in attention matrix that maximizes cost:\n",
    "$$(j_i) = \\mathsf{argmax} (Cost) = \\mathsf{argmax} \\prod_{i=1}^{N_{frames}} A_{i j_i}$$\n",
    "where:\n",
    "$$j_{i+1} = j\\;or\\;j+1$$\n",
    "$$j_1 = 1$$\n",
    "$$j_{N_{frames}} = N_{graphemes}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c3ab9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path(dp):\n",
    "    n_frames, n_graphemes = dp.shape\n",
    "    path = []\n",
    "    path.append(n_graphemes - 1)\n",
    "    for i in range(n_frames - 2, 0, -1):\n",
    "        last_grapheme = path[-1]\n",
    "        path.append(last_grapheme - np.argmax([dp[i][last_grapheme], dp[i][last_grapheme - 1]]))\n",
    "    path.append(0)\n",
    "    return path[::-1]\n",
    "\n",
    "\n",
    "def dp_alignment(attention):\n",
    "    \"\"\"\n",
    "    Here you are given with an attention prob matrix\n",
    "        with a shape of N_frames X N_graphemes\n",
    "        \n",
    "    You should compute the optimal way according to the formula above with DP.\n",
    "    With the optimal way (sequence j_i) computed you should return durations.\n",
    "    So, duration of j-th gratheme == #(i: j_i == j) -- number of frames at which\n",
    "        our optimal way is 'stuck' at the grapheme.\n",
    "    These durations, as an integer numpy array should be returned from the function.\n",
    "    \"\"\" \n",
    "\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!\n",
    "    # INSERT YOUR CODE HERE!\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!\n",
    "    n_frames, n_graphemes = attention.shape\n",
    "    dp = np.ones_like(attention) * -np.inf\n",
    "    dp[0][0] = np.log(attention[0][0])\n",
    "    for i in range(1, n_frames):\n",
    "        for j in range(min(i + 2, n_graphemes)):\n",
    "            dp_i_minus_1_j = dp[i - 1][j] if i - 1 >= 0 else -np.inf\n",
    "            dp_i_minus_1_j_minus_1 = dp[i - 1][j - 1] if i - 1 >= 0 and j - 1 >= 0 else -np.inf\n",
    "            dp[i][j] = np.log(attention[i][j]) + max(dp_i_minus_1_j, dp_i_minus_1_j_minus_1)\n",
    "    max_path = extract_path(dp)\n",
    "    \n",
    "    durations = np.zeros(n_graphemes,)\n",
    "    for j_i in max_path:\n",
    "        durations[j_i] += 1\n",
    "    return durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e9ccfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_durations(taco_checkpoint_path, data_path, hparams, batch_size=8):\n",
    "    taco_checkpoint = torch.load(taco_checkpoint_path)\n",
    "    taco_model = Tacotron2(**taco_checkpoint['config']).to(device)\n",
    "    load_from_checkpoint(taco_model, taco_checkpoint['state_dict'])\n",
    "    taco_model.eval()\n",
    "\n",
    "    taco_dataset = TacoDataset(data_path)\n",
    "    taco_collate = TacoCollate(hparams.n_mel_channels)\n",
    "    taco_dataloader = DataLoader(taco_dataset, batch_size=batch_size, collate_fn=taco_collate)\n",
    "    \n",
    "    for batch, file_ids in tqdm.tqdm(taco_dataloader):\n",
    "        with torch.no_grad():\n",
    "            batch = [elem.to(device) \n",
    "                     if torch.is_tensor(elem)\n",
    "                     else elem\n",
    "                     for elem in batch]\n",
    "            _, text_lengths, _, _, mel_lengths = batch\n",
    "            \n",
    "            *_, alignments = taco_model.forward(batch)\n",
    "            alignments = alignments.cpu().numpy()\n",
    "            \n",
    "            for j, ali in enumerate(alignments):\n",
    "                dur_path = os.path.join(data_path, file_ids[j] + \".dur\")\n",
    "                \n",
    "                ali = ali[: mel_lengths[j], : text_lengths[j]]\n",
    "                durations = dp_alignment(ali)\n",
    "                np.save(dur_path, durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc4591e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_durations(taco2_path, prepr_data_path, hparams, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5baacc8",
   "metadata": {},
   "source": [
    "Check yourself:\n",
    "\n",
    "Average \"a\" duration in the corpus should not be far from 6.782268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c60efcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncnt = 0\\nsum = 0\\nfor i, _ in tqdm.tqdm(fl):\\n    text = np.load(os.path.join(prepr_data_path, i + \".text.npy\"))\\n    dur = np.load(os.path.join(prepr_data_path, i + \".dur.npy\"))\\n    for c, d in zip(text, dur):\\n        if c == 38:\\n            cnt += 1\\n            sum += d\\nmean_a_duration = sum/cnt\\nprint(\"Mean \\'a\\' duration in the corpus is {} frames\".format(mean_a_duration))\\nassert np.abs(mean_a_duration - 6.7823) < 0.01\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cnt = 0\n",
    "sum = 0\n",
    "for i, _ in tqdm.tqdm(fl):\n",
    "    text = np.load(os.path.join(prepr_data_path, i + \".text.npy\"))\n",
    "    dur = np.load(os.path.join(prepr_data_path, i + \".dur.npy\"))\n",
    "    for c, d in zip(text, dur):\n",
    "        if c == 38:\n",
    "            cnt += 1\n",
    "            sum += d\n",
    "mean_a_duration = sum/cnt\n",
    "print(\"Mean 'a' duration in the corpus is {} frames\".format(mean_a_duration))\n",
    "assert np.abs(mean_a_duration - 6.7823) < 0.01\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c6b00",
   "metadata": {},
   "source": [
    "### pitch extraction\n",
    "\n",
    "here we use parselmouth pitch extractor, pad and store the extracted f0 values into npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9775696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_pad(vec, l):\n",
    "    assert np.abs(vec.shape[0] - l) <= 3\n",
    "    vec = vec[:l]\n",
    "    if vec.shape[0] < l:\n",
    "        vec = np.pad(vec, pad_width=(0, l - vec.shape[0]))\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45b54901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pitch(wav_path, durs):\n",
    "    durs = durs.astype(int)\n",
    "    mel_len = durs.sum().astype(int)\n",
    "    durs_cum = np.cumsum(np.pad(durs, (1, 0))).astype(int)\n",
    "\n",
    "    snd = parselmouth.Sound(wav_path)\n",
    "    pitch = snd.to_pitch(time_step=snd.duration / (mel_len + 3)).selected_array[\"frequency\"]\n",
    "    assert np.abs(mel_len - pitch.shape[0]) <= 1.0\n",
    "\n",
    "    # Average pitch over characters\n",
    "    pitch_char = np.zeros((durs.shape[0],), dtype=float)\n",
    "    for idx, a, b in zip(range(int(mel_len)), durs_cum[:-1], durs_cum[1:]):\n",
    "        values = pitch[a:b][np.where(pitch[a:b] != 0.0)[0]]\n",
    "        pitch_char[idx] = np.mean(values) if len(values) > 0 else 0.0\n",
    "\n",
    "    pitch_char = maybe_pad(pitch_char, len(durs))\n",
    "\n",
    "    return pitch_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "193b5f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pitch(wavs_path, preproc_data_path):\n",
    "    for filename in tqdm.tqdm(os.listdir(wavs_path)):\n",
    "        file_id = re.search(r\"(.*).wav\", filename).group(1)\n",
    "        wav_path = os.path.join(wavs_path, filename)\n",
    "        dur_path = os.path.join(preproc_data_path, file_id + \".dur.npy\")\n",
    "        pitch_path = os.path.join(preproc_data_path, file_id + \".pt.npy\")\n",
    "\n",
    "        dur = np.load(dur_path)\n",
    "        pitch = calculate_pitch(str(wav_path), dur)\n",
    "        np.save(pitch_path, pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1470b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13100/13100 [04:01<00:00, 54.25it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_pitch(os.path.join(ljspeech_data_path, \"wavs\"), prepr_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4383f5",
   "metadata": {},
   "source": [
    "## Data utils\n",
    "\n",
    "some helper classes for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9eb79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LJSPEECH_MEAN = 218.44949768191876\n",
    "LJSPEECH_STD = 64.85769765940508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6bd6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastPitchDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Provides random access to the samples\n",
    "    returns data in the form of (text, mel, duration, pitch)\n",
    "    \"\"\"\n",
    "    def __init__(self, directory, filelist_path):\n",
    "        super(FastPitchDataset, self).__init__()\n",
    "        self.directory = directory\n",
    "        self.mel_paths = []\n",
    "        self.pitch_paths = []\n",
    "        self.dur_paths = []\n",
    "        self.text_paths = []\n",
    "\n",
    "        with open(filelist_path, \"r\") as filelist:\n",
    "            file_ids = filelist.readlines()\n",
    "        file_ids = [elem.strip() for elem in file_ids] \n",
    "        \n",
    "        for file_id in file_ids:\n",
    "            text_path = os.path.join(directory, file_id + \".text.npy\")\n",
    "            self.text_paths.append(text_path)\n",
    "            self.pitch_paths.append(text_path.replace(\".text.npy\", \".pt.npy\"))\n",
    "            self.dur_paths.append(text_path.replace(\".text.npy\", \".dur.npy\"))\n",
    "            self.mel_paths.append(text_path.replace(\".text.npy\", \".mel.npy\"))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.IntTensor(np.load(self.text_paths[idx]))\n",
    "        mel = torch.FloatTensor(np.load(self.mel_paths[idx]))\n",
    "        dur = torch.FloatTensor(np.load(self.dur_paths[idx]))\n",
    "        pitch = torch.FloatTensor(np.load(self.pitch_paths[idx]))\n",
    "        \n",
    "        pitch[pitch != 0] = (pitch[pitch != 0] - LJSPEECH_MEAN) / LJSPEECH_STD\n",
    "\n",
    "        return (text, mel, dur, pitch)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_paths)\n",
    "    \n",
    "    \n",
    "class FastPitchCollate:\n",
    "    \"\"\"\n",
    "    Groups and pads the date to the batches,\n",
    "    adds lengths of texts and mels tensors to the batch info\n",
    "    \"\"\"\n",
    "    def __init__(self, n_mel_channels):\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        max_text_length = max([text.size(0) for text, *_ in batch])\n",
    "        max_mel_length = max([mel.size(1) for _, mel, *_ in batch])\n",
    "    \n",
    "        texts_padded = torch.LongTensor(len(batch), max_text_length).zero_()\n",
    "        text_lengths = torch.LongTensor(len(batch)).zero_()\n",
    "        mels_padded = torch.FloatTensor(len(batch), self.n_mel_channels, max_mel_length).zero_()\n",
    "        mel_lengths = torch.LongTensor(len(batch)).zero_()\n",
    "        dur_padded = torch.zeros_like(texts_padded, dtype=batch[0][2].dtype)\n",
    "        pitch_padded = torch.zeros(dur_padded.size(0), dur_padded.size(1), dtype=batch[0][3].dtype)\n",
    "        \n",
    "        for i, (text, mel, dur, pitch) in enumerate(batch):\n",
    "            texts_padded[i, :text.size(0)] = text\n",
    "            text_lengths[i] = text.size(0)\n",
    "            mels_padded[i, :, :mel.size(1)] = mel\n",
    "            mel_lengths[i] = mel.size(1)\n",
    "            dur_padded[i, :dur.shape[0]] = dur\n",
    "            pitch_padded[i, :pitch.shape[0]] = pitch\n",
    "\n",
    "        return (texts_padded,\n",
    "            text_lengths,\n",
    "            mels_padded,\n",
    "            mel_lengths,\n",
    "            dur_padded,\n",
    "            pitch_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d73afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_device(batch, device):\n",
    "    \"\"\"\n",
    "    Pushes all the numeric data to GPU\n",
    "    Splits input and output data of the model\n",
    "    \"\"\"\n",
    "    (texts, \n",
    "     text_lengths, \n",
    "     mels, \n",
    "     mel_lengths, \n",
    "     dur,  \n",
    "     pitch\n",
    "    ) = [tensor.to(device) if tensor is not None\n",
    "               else None\n",
    "               for tensor in batch]\n",
    "    \n",
    "    x = (texts, text_lengths, mels, mel_lengths, dur, pitch)\n",
    "    y = (mels, dur, text_lengths, pitch)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f44496ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_from_lens(lengths, max_len=None):\n",
    "    \"\"\"\n",
    "    With mels' or texts' lengths given creates a 0-1 mask for losses\n",
    "    \"\"\"\n",
    "    if max_len is None:\n",
    "        max_len = torch.max(lengths).item()\n",
    "    ids = torch.arange(0, max_len, device=lengths.device)\n",
    "    mask = (ids < lengths.unsqueeze(1))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc8655",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "044d097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastPitchLoss(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super(FastPitchLoss, self).__init__()\n",
    "        self.dur_predictor_loss_scale = hparams.dur_predictor_loss_scale\n",
    "        self.pitch_predictor_loss_scale = hparams.pitch_predictor_loss_scale\n",
    "\n",
    "    def forward(self, model_out, targets, meta_agg='sum'):\n",
    "        \"\"\"\n",
    "        Here you get\n",
    "        * model_out, which is a tuple of\n",
    "            mel_output, decoding_mask, predicted_duration, log_pred_duration and predicted_pitch\n",
    "        * targets, which is also a tuple:\n",
    "            mel_target, duration_target, duration_lengths, pitch_target\n",
    "        \n",
    "        you should compute loss - a weighted (look at constructor and hparams) sum of\n",
    "            (masked) l2 mels, pitch and durations losses\n",
    "        also, to draw graphs you should return the components separately - in a dict meta - kv struct with keys:\n",
    "            * loss\n",
    "            * mel_loss\n",
    "            * duration_predictor_loss\n",
    "            * pitch_loss\n",
    "            * duration_l1_error\n",
    "        \"\"\"\n",
    "        mel_out, dec_mask, dur_pred, log_dur_pred, pitch_pred = model_out\n",
    "        mel_tgt, dur_tgt, dur_lens, pitch_tgt = targets\n",
    "        mel_tgt.requires_grad = False\n",
    "        # (B,H,T) => (B,T,H)\n",
    "        mel_tgt = mel_tgt.transpose(1, 2)\n",
    "\n",
    "        dur_mask = mask_from_lens(dur_lens, max_len=dur_tgt.size(1))\n",
    "\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        log_dur_tgt = torch.log(dur_tgt.float() + 1)\n",
    "        loss_fn = F.mse_loss\n",
    "        dur_pred_loss = loss_fn(log_dur_pred, log_dur_tgt, reduction='none')\n",
    "        dur_pred_loss = (dur_pred_loss * dur_mask).sum() / dur_mask.sum()\n",
    "\n",
    "        ldiff = mel_tgt.size(1) - mel_out.size(1)\n",
    "        mel_out = F.pad(mel_out, (0, 0, 0, ldiff, 0, 0), value=0.0)\n",
    "        mel_mask = mel_tgt.ne(0).float()\n",
    "        loss_fn = F.mse_loss\n",
    "        mel_loss = loss_fn(mel_out, mel_tgt, reduction='none')\n",
    "        mel_loss = (mel_loss * mel_mask).sum() / mel_mask.sum()\n",
    "\n",
    "        ldiff = pitch_tgt.size(1) - pitch_pred.size(1)\n",
    "        pitch_pred = F.pad(pitch_pred, (0, ldiff, 0, 0, ), value=0.0)\n",
    "        pitch_loss = F.mse_loss(pitch_tgt, pitch_pred, reduction='none')\n",
    "        pitch_loss = (pitch_loss * dur_mask).sum() / dur_mask.sum()\n",
    "    \n",
    "        loss = (\n",
    "            mel_loss\n",
    "            + pitch_loss * self.pitch_predictor_loss_scale\n",
    "            + dur_pred_loss * self.dur_predictor_loss_scale\n",
    "        )\n",
    "\n",
    "        meta = {\n",
    "            'loss':                    loss.clone().detach(),\n",
    "            'mel_loss':                mel_loss.clone().detach(),\n",
    "            'duration_predictor_loss': dur_pred_loss.clone().detach(),\n",
    "            'pitch_loss':              pitch_loss.clone().detach(),\n",
    "            'duration_l1_error':       (torch.abs(dur_pred - dur_tgt).sum() / dur_mask.sum()).detach(),\n",
    "        }\n",
    "\n",
    "        assert meta_agg in ('sum', 'mean')\n",
    "        if meta_agg == 'sum':\n",
    "            # !!!!!!!!!!!!!!!!!!!!!!\n",
    "            # INSERT YOUR CODE HERE!\n",
    "            # !!!!!!!!!!!!!!!!!!!!!!\n",
    "            bsz = mel_out.size(0)\n",
    "            meta = {k: v * bsz for k,v in meta.items()}\n",
    "            \n",
    "        return loss, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c50e6bd",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a03c46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastpitchLogger:\n",
    "    \"\"\"\n",
    "    Logger. Saves/loads model, flushes on validation, can log gradients (useful for debugging the core AM model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logdir, hparams):\n",
    "        self.eval_interval = hparams.eval_interval\n",
    "        self.global_checkpoint_coef = hparams.global_checkpoint_coef\n",
    "        self.logdir = logdir\n",
    "        \n",
    "        self.train_logger = SummaryWriter(os.path.join(logdir, 'train'))\n",
    "        self.val_logger = SummaryWriter(os.path.join(logdir, 'val'))\n",
    "\n",
    "    def log_grads(self, step, model):\n",
    "            norms = [p.grad.norm().item() for p in model.parameters()\n",
    "                     if p.grad is not None]\n",
    "            for stat in ('max', 'min', 'mean'):\n",
    "                self.train_logger.add_scalar(f'grad_{stat}', getattr(np, stat)(norms), step)\n",
    "\n",
    "    def log(self, logger, step, meta):\n",
    "        for k, v in meta.items():\n",
    "            logger.add_scalar(k, \n",
    "                            v.item() if hasattr(v, 'item') else v, \n",
    "                            step)\n",
    "    \n",
    "    def log_training(self, step, meta):\n",
    "        print(f\"train : {meta['step']}  loss {meta['loss']:.4f}\")\n",
    "        self.log(self.train_logger, step, meta)\n",
    "\n",
    "    def log_validation(self, step, meta):\n",
    "        print(f\"val : {meta['step']}  loss {meta['loss']}\")\n",
    "        self.log(self.val_logger, step, meta)\n",
    "            \n",
    "        for w in self.train_logger.all_writers.values():\n",
    "            w.flush()\n",
    "        for w in self.val_logger.all_writers.values():\n",
    "            w.flush()\n",
    "\n",
    "    def step_to_path(self, step):\n",
    "        return os.path.join(self.logdir, f\"{step:07d}_snapshot.pth\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_last_checkpoint(logdir):\n",
    "        steps = [\n",
    "            int(filename.replace('_snapshot.pth', ''))\n",
    "            for filename in os.listdir(logdir)\n",
    "            if '_snapshot.pth' in filename\n",
    "        ]\n",
    "\n",
    "        if len(steps) == 0:\n",
    "            return {\n",
    "                'step': 0,\n",
    "                'state_dict': None\n",
    "            }\n",
    "\n",
    "        return torch.load(os.path.join(logdir, f\"{max(steps):07d}_snapshot.pth\"))\n",
    "\n",
    "    def save_checkpoint(self, model, optimizer, step, hparams):\n",
    "        checkpoint = {\n",
    "            'step': step,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'params': hparams.__dict__\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, self.step_to_path(step))\n",
    "\n",
    "        # remove unnecessary snapshots\n",
    "        if step > self.eval_interval and (step - self.eval_interval) % (self.global_checkpoint_coef * self.eval_interval):\n",
    "            os.remove(self.step_to_path(step - self.eval_interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a67f2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d465b8",
   "metadata": {},
   "source": [
    "It is highly recommended to go briefly through the lecture\n",
    "and/or FastPitch paper https://arxiv.org/abs/2006.06873 before doing the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e96e05d",
   "metadata": {},
   "source": [
    "### FFTransformer \n",
    "\n",
    "Here is the core part of FastPitch model.\n",
    "You should implement self-attention block here.\n",
    "\n",
    "Note: the classes below are provided for your convenience. You are free to re-write everything in your own style if your architecture will work and you will implement in your code main idea of FastPitch (FFT encoder and decoder, duration and pitch prediction in between)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa710c3",
   "metadata": {},
   "source": [
    "Here you need to implement sin-cos pos embeddings:\n",
    "$$PE[p, 2 i] = sin\\left(\\frac{p}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE[p, 2 i + 1] = cos \\left(\\frac{p}{10000^{2i/d_{model}}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "795031c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sin and cos positional embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        self.demb = d_model\n",
    "        inv_freq = 1 / (10000 ** (torch.arange(0.0, self.demb, 2.0) / self.demb))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    \"\"\"\n",
    "    * pos_seq - 1...N array\n",
    "    \"\"\"\n",
    "    def forward(self, pos_seq, bsz=None):\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        sinusoid_inp = torch.matmul(torch.unsqueeze(pos_seq, -1),\n",
    "                                    torch.unsqueeze(self.inv_freq, 0))\n",
    "        pos_emb = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=1)\n",
    "        if bsz is not None:\n",
    "            return pos_emb[None, :, :].expand(bsz, -1, -1)\n",
    "        else:\n",
    "            return pos_emb[None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3bae6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseConvFF(nn.Module):\n",
    "    \"\"\"\n",
    "    This position-wise convolutional layer\n",
    "    It is a sequence:\n",
    "    * Conv1d, d_model -> d_inner\n",
    "    * ReLU\n",
    "    * Conv1d, d_inner -> d_model\n",
    "    * Dropout\n",
    "    * Residual (+input)\n",
    "    * LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_inner, kernel_size, dropout):\n",
    "        super(PositionwiseConvFF, self).__init__()\n",
    "        self.CoreNet = nn.Sequential(\n",
    "            nn.Conv1d(d_model, d_inner, kernel_size, 1, (kernel_size // 2)),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout),  # worse convergence\n",
    "            nn.Conv1d(d_inner, d_model, kernel_size, 1, (kernel_size // 2)),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # positionwise feed-forward\n",
    "        core_out = inp.transpose(1, 2)\n",
    "        core_out = self.CoreNet(core_out)\n",
    "        core_out = core_out.transpose(1, 2)\n",
    "\n",
    "        # residual connection + layer normalization\n",
    "        output = self.layer_norm(inp + core_out).to(inp.dtype)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bb329a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttn(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0.1):\n",
    "        super(MultiHeadAttn, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_head\n",
    "        self.scale = 1 / (d_head ** 0.5)\n",
    "\n",
    "        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.dropatt = nn.Dropout(dropatt)\n",
    "        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inp, attn_mask=None):\n",
    "        return self._forward(inp, attn_mask)\n",
    "\n",
    "    def _forward(self, inp, attn_mask=None):\n",
    "        residual = inp\n",
    "\n",
    "        n_head, d_head = self.n_head, self.d_head\n",
    "\n",
    "        head_q, head_k, head_v = torch.chunk(self.qkv_net(inp), 3, dim=2)\n",
    "        head_q = head_q.view(inp.size(0), inp.size(1), n_head, d_head)\n",
    "        head_k = head_k.view(inp.size(0), inp.size(1), n_head, d_head)\n",
    "        head_v = head_v.view(inp.size(0), inp.size(1), n_head, d_head)\n",
    "\n",
    "        q = head_q.permute(0, 2, 1, 3).reshape(-1, inp.size(1), d_head)\n",
    "        k = head_k.permute(0, 2, 1, 3).reshape(-1, inp.size(1), d_head)\n",
    "        v = head_v.permute(0, 2, 1, 3).reshape(-1, inp.size(1), d_head)\n",
    "\n",
    "        attn_score = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn_score.mul_(self.scale)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.unsqueeze(1).to(attn_score.dtype)\n",
    "            attn_mask = attn_mask.repeat(n_head, attn_mask.size(2), 1)\n",
    "            attn_score.masked_fill_(attn_mask.to(torch.bool), -float('inf'))\n",
    "\n",
    "        attn_prob = F.softmax(attn_score, dim=2)\n",
    "        attn_prob = self.dropatt(attn_prob)\n",
    "        attn_vec = torch.bmm(attn_prob, v)\n",
    "\n",
    "        attn_vec = attn_vec.view(n_head, inp.size(0), inp.size(1), d_head)\n",
    "        attn_vec = attn_vec.permute(1, 2, 0, 3).contiguous().view(\n",
    "            inp.size(0), inp.size(1), n_head * d_head)\n",
    "\n",
    "        # linear projection\n",
    "        attn_out = self.o_net(attn_vec)\n",
    "        attn_out = self.drop(attn_out)\n",
    "\n",
    "        # residual connection + layer normalization\n",
    "        output = self.layer_norm(residual + attn_out)\n",
    "\n",
    "        output = output.to(attn_out.dtype)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf7eb7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_head, d_inner, kernel_size, dropout,\n",
    "                 **kwargs):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.dec_attn = MultiHeadAttn(n_head, d_model, d_head, dropout, **kwargs)\n",
    "        self.pos_ff = PositionwiseConvFF(d_model, d_inner, kernel_size, dropout)\n",
    "\n",
    "    def forward(self, dec_inp, mask=None):\n",
    "        output = self.dec_attn(dec_inp, attn_mask=~mask.squeeze(2))\n",
    "        output *= mask\n",
    "        output = self.pos_ff(output)\n",
    "        output *= mask\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4b2d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Here, you should implement the sequence:\n",
    "    * embeddings of the categorical input, if embed_input is True\n",
    "    * dropout(input + embeddings), if dropemb is not 0\n",
    "    * n_layers of (multi-head self-attention + dropout + layernorm + pos-wise convFF)\n",
    "    * all this should be masked according to the lengths\n",
    "    \n",
    "    Parameters:\n",
    "    * n_symbols - number of symbols (in case of cat input)\n",
    "    * n_layer - self-att layers\n",
    "    * d_model - outer dim, n_embedding for graphemes, n_mel for audio\n",
    "    * n_head - number of attention heads\n",
    "    * d_head - size of each head\n",
    "    * kernel size - pos-wise convolution kernel size\n",
    "    * dropout - DO after the self-attention\n",
    "    * dropatt - DO at self-attention probs\n",
    "    * dropemb - DO after the positional embeddings addition\n",
    "    * embed_input - if true, input is treated as categorical and additional nn.Embeddings should be added\n",
    "    * pad_idx - only for embed_input = true: special value categorical input are padded with\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_symbols,\n",
    "        n_layer,\n",
    "        n_head,\n",
    "        d_model,\n",
    "        d_head,\n",
    "        d_inner,\n",
    "        kernel_size,\n",
    "        dropout,\n",
    "        dropatt,\n",
    "        dropemb=0.0,\n",
    "        embed_input=True,\n",
    "        pad_idx=0\n",
    "    ):\n",
    "        super(FFTransformer, self).__init__()\n",
    "\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_head\n",
    "        self.padding_idx = pad_idx\n",
    "\n",
    "        if embed_input:\n",
    "            self.word_emb = nn.Embedding(n_symbols, d_model, padding_idx=self.padding_idx)\n",
    "        else:\n",
    "            self.word_emb = None\n",
    "\n",
    "        self.pos_emb = PositionalEmbedding(self.d_model)\n",
    "        self.drop = nn.Dropout(dropemb)\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(n_layer):\n",
    "            self.layers.append(\n",
    "                TransformerLayer(n_head, d_model, d_head, d_inner, kernel_size, dropout, dropatt=dropatt)\n",
    "            )\n",
    "\n",
    "    def forward(self, dec_inp, mel_lens=None):\n",
    "        \"\"\"\n",
    "        * dec_inp - input, continuous or categorical\n",
    "        * mel_lens - for embed_input = False: lengths of input, need for padding\n",
    "        Return:\n",
    "        * tuple: processed output and padding bitmask\n",
    "        \"\"\"\n",
    "        if self.word_emb is None:\n",
    "            inp = dec_inp\n",
    "            mask = mask_from_lens(mel_lens).unsqueeze(2)\n",
    "        else:\n",
    "            inp = self.word_emb(dec_inp)\n",
    "            mask = (dec_inp != self.padding_idx).unsqueeze(2)\n",
    "        \n",
    "        pos_seq = torch.arange(inp.size(1), device=inp.device).to(inp.dtype)\n",
    "        pos_emb = self.pos_emb(pos_seq) * mask\n",
    "\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        #print(inp.shape)\n",
    "        #print(pos_emb.shape)\n",
    "        out = self.drop(inp + pos_emb)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask=mask)\n",
    "\n",
    "        return out, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6826640b",
   "metadata": {},
   "source": [
    "## FastPitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8bd5a9",
   "metadata": {},
   "source": [
    "#### Temporal Predictor \n",
    "(predicts a single float per each temporal location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1901fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvReLUNorm(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, dropout=0.0):\n",
    "        super(ConvReLUNorm, self).__init__()\n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size,\n",
    "                                    padding=(kernel_size // 2))\n",
    "        self.norm = torch.nn.LayerNorm(out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, signal):\n",
    "        out = F.relu(self.conv(signal))\n",
    "        out = self.norm(out.transpose(1, 2)).transpose(1, 2)\n",
    "        return self.dropout(out)\n",
    "    \n",
    "\n",
    "class TemporalPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Regressor head\n",
    "    A couple of conv+reLU+layernorm blocks + projection to answer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, filter_size, kernel_size, dropout, n_layers=2):\n",
    "        super(TemporalPredictor, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            *[\n",
    "                ConvReLUNorm(\n",
    "                    input_size if i == 0 else filter_size,\n",
    "                    filter_size,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "                for i in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc = nn.Linear(filter_size, 1, bias=True)\n",
    "\n",
    "    def forward(self, enc_out, enc_out_mask):\n",
    "        out = enc_out * enc_out_mask\n",
    "        out = self.layers(out.transpose(1, 2)).transpose(1, 2)\n",
    "        out = self.fc(out) * enc_out_mask\n",
    "        return out.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abaf5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regulate_len(durations, enc_out, pace=1.0, mel_max_len=None):\n",
    "    \"\"\"\n",
    "    Here you should write the part with upsample:\n",
    "    Parameters:\n",
    "    * durations - predicted/gt durations array\n",
    "    * enc_out - encoder embeddings you need to upsample\n",
    "    Returns tuple:\n",
    "    * encoder upsampled states\n",
    "    * frame_lengths - sums over T of durations\n",
    "    \n",
    "    \"\"\"\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!\n",
    "    # INSERT YOUR CODE HERE!\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!\n",
    "    \"\"\"If target=None, then predicted durations are applied\"\"\"\n",
    "    dtype = enc_out.dtype\n",
    "    reps = durations.float() / pace\n",
    "    reps = (reps + 0.5).long()\n",
    "    dec_lens = reps.sum(dim=1)\n",
    "\n",
    "    max_len = dec_lens.max()\n",
    "    reps_cumsum = torch.cumsum(F.pad(reps, (1, 0, 0, 0), value=0.0), dim=1)[:, None, :]\n",
    "    reps_cumsum = reps_cumsum.to(dtype)\n",
    "\n",
    "    range_ = torch.arange(max_len).to(enc_out.device)[None, :, None]\n",
    "    mult = ((reps_cumsum[:, :, :-1] <= range_) &\n",
    "            (reps_cumsum[:, :, 1:] > range_))\n",
    "    mult = mult.to(dtype)\n",
    "    enc_rep = torch.matmul(mult, enc_out)\n",
    "\n",
    "    if mel_max_len:\n",
    "        enc_rep = enc_rep[:, :mel_max_len]\n",
    "        dec_lens = torch.clamp_max(dec_lens, mel_max_len)\n",
    "    return enc_rep, dec_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0bc493eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastPitch(nn.Module):\n",
    "    \"\"\"\n",
    "    Here you need to construct and infer FP net\n",
    "    Note that predicted f0 should be feeded into decoder through convolution with a small kernel\n",
    "    \"\"\"\n",
    "    def __init__(self, hparams : HParamsFastpitch):\n",
    "        super(FastPitch, self).__init__()\n",
    "\n",
    "        self.encoder = FFTransformer(\n",
    "            n_symbols=hparams.n_symbols,\n",
    "            n_layer=hparams.in_fft_n_layers,\n",
    "            n_head=hparams.in_fft_n_heads,\n",
    "            d_model=hparams.symbols_embedding_dim,\n",
    "            d_head=hparams.in_fft_d_head,\n",
    "            d_inner=4 * hparams.symbols_embedding_dim,\n",
    "            kernel_size=hparams.in_fft_conv1d_kernel_size,\n",
    "            dropout=hparams.p_in_fft_dropout,\n",
    "            dropatt=hparams.p_in_fft_dropatt,\n",
    "            dropemb=hparams.p_in_fft_dropemb,\n",
    "            embed_input=True,\n",
    "            pad_idx=hparams.pad_idx)\n",
    "\n",
    "        self.duration_predictor = TemporalPredictor(\n",
    "            input_size=hparams.symbols_embedding_dim,\n",
    "            filter_size=hparams.dur_predictor_filter_size,\n",
    "            kernel_size=hparams.dur_predictor_kernel_size,\n",
    "            dropout=hparams.p_dur_predictor_dropout,\n",
    "            n_layers=hparams.dur_predictor_n_layers,\n",
    "        )\n",
    "\n",
    "        self.decoder = FFTransformer(\n",
    "            n_symbols=hparams.n_symbols,\n",
    "            n_layer=hparams.out_fft_n_layers,\n",
    "            n_head=hparams.out_fft_n_heads,\n",
    "            d_model=hparams.symbols_embedding_dim,\n",
    "            d_head=hparams.out_fft_d_head,\n",
    "            d_inner=4 * hparams.symbols_embedding_dim,\n",
    "            kernel_size=hparams.out_fft_conv1d_kernel_size,\n",
    "            dropout=hparams.p_out_fft_dropout,\n",
    "            dropatt=hparams.p_out_fft_dropatt,\n",
    "            dropemb=hparams.p_out_fft_dropemb,\n",
    "            embed_input=False,\n",
    "            pad_idx=hparams.pad_idx\n",
    "        )\n",
    "\n",
    "        self.pitch_predictor = TemporalPredictor(\n",
    "            input_size=hparams.symbols_embedding_dim,\n",
    "            filter_size=hparams.pitch_predictor_filter_size,\n",
    "            kernel_size=hparams.pitch_predictor_kernel_size,\n",
    "            dropout=hparams.p_pitch_predictor_dropout,\n",
    "            n_layers=hparams.pitch_predictor_n_layers,\n",
    "        )\n",
    "        \n",
    "        \"\"\"\n",
    "        Here you need to declare all additional layers you need\n",
    "        Don't forget about GST module\n",
    "        \"\"\"\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        self.pitch_emb = nn.Conv1d(\n",
    "            1, hparams.symbols_embedding_dim,\n",
    "            kernel_size=hparams.pitch_predictor_kernel_size,\n",
    "            padding=int((hparams.pitch_predictor_kernel_size - 1) / 2))\n",
    "\n",
    "        self.projection = nn.Linear(hparams.symbols_embedding_dim, hparams.n_mel_channels, bias=True)\n",
    "\n",
    "        # self.gst = GST(hparams)\n",
    "\n",
    "    def forward(self, inputs, use_gt_durations=True, use_gt_pitch=True, max_duration=75):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        * inputs - tuple of:\n",
    "            * text - texts\n",
    "            * text_lengths - lengths for padding\n",
    "            * mel_tgt - targets for melspec\n",
    "            * mel_lengths - for padding\n",
    "            * dur_tgt - gt duration\n",
    "            * pitch tgt - gt pitch\n",
    "        * use_gt_durations and use_gt_pitch - True for teache-forcing (train and validation estimation)\n",
    "        * max_duration - maximum possible duration\n",
    "        \"\"\"\n",
    "        text, text_lengths, mel_tgt, mel_lengths, dur_tgt, pitch_tgt = inputs\n",
    "\n",
    "        \"\"\"\n",
    "        Here you should wrtie the code for ecoder\n",
    "        Internal state should be (before the upsampling):\n",
    "            enc_state = encoded_text + gst_style + conv(pitch)\n",
    "        \n",
    "        \"\"\"\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        encoder_output, enc_mask = self.encoder(text)\n",
    "        enc_and_style = encoder_output # + gst_style\n",
    "    \n",
    "        pitch_pred = self.pitch_predictor(enc_and_style, enc_mask)\n",
    "    \n",
    "        \"\"\"\n",
    "        Duration is predicted in log-scale\n",
    "        \"\"\"\n",
    "        log_dur_pred = self.duration_predictor(enc_and_style, enc_mask)\n",
    "        dur_pred = torch.clamp(torch.exp(log_dur_pred) - 1, 0, max_duration)\n",
    "        \n",
    "        if use_gt_pitch and pitch_tgt is not None:\n",
    "            pitch_conv = self.pitch_emb(pitch_tgt.unsqueeze(1)).permute(0, 2, 1)\n",
    "        else:\n",
    "            pitch_conv = self.pitch_emb(pitch_pred.unsqueeze(1)).permute(0, 2, 1)\n",
    "    \n",
    "        enc_state = enc_and_style + pitch_conv\n",
    "\n",
    "        upsampled, dec_lens = regulate_len(dur_tgt if use_gt_durations else dur_pred, enc_state)\n",
    "    \n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        dec_out, dec_mask = self.decoder(upsampled, dec_lens)\n",
    "        mel_out = self.projection(dec_out)\n",
    "        \n",
    "        return mel_out, dec_mask, dur_pred, log_dur_pred, pitch_pred\n",
    "\n",
    "    def infer(self, inputs, gst_estimator):\n",
    "        \"\"\"\n",
    "        Here you should write the code for FP inference\n",
    "        Inputs:\n",
    "        * inputs - text and texts_lengths\n",
    "        * gst_estimator - nn.Module that predicts style from (encoded_text, text_lengths)\n",
    "        Returns:\n",
    "        * mel_output\n",
    "        \"\"\"\n",
    "        text, text_lengths = inputs\n",
    "        \n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        enc_out, enc_mask = self.encoder(text)\n",
    "        \n",
    "        gst_pred = gst_estimator.forward(encoder_output, text_lengths)\n",
    "        enc_and_style = encoder_output + gst_pred.unsqueeze(1)\n",
    "        \n",
    "        log_dur_pred = self.duration_predictor(enc_and_style, enc_mask)\n",
    "        dur_pred = torch.clamp(torch.exp(log_dur_pred) - 1, min=0)\n",
    "\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        pitch_pred = self.pitch_predictor(enc_out, enc_mask).unsqueeze(1)\n",
    "        pitch_conv = self.pitch_emb(pitch_pred)\n",
    "        \n",
    "        enc_state = enc_and_style + pitch_conv.transpose(1, 2)\n",
    "        len_regulated, dec_lens = regulate_len(dur_pred, enc_state)\n",
    "        \n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        dec_out, dec_mask = self.decoder(len_regulated, dec_lens)\n",
    "        mel_out = self.projection(dec_out)\n",
    "        \n",
    "        return mel_out.transpose(1, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b8da6c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9998051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(directory, train_filelist, val_filelist, hparams):\n",
    "    train_dataset = FastPitchDataset(directory, train_filelist)\n",
    "    valid_dataset = FastPitchDataset(directory, val_filelist)\n",
    "\n",
    "    collate_fn = FastPitchCollate(hparams.n_mel_channels)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=hparams.batch_size,\n",
    "        num_workers=16,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=hparams.batch_size,\n",
    "        num_workers=8,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffb2425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, val_loader, device, use_gt_durations=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    * model - FastPitch\n",
    "    * criterion - loss operator\n",
    "    * val_loader - loader for validation data\n",
    "    * device - torch GPU id\n",
    "    * use_gt_durations - if use ground truth for durations\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_meta = defaultdict(float)\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            x, y = batch_to_device(batch, device)\n",
    "            \n",
    "            # !!!!!!!!!!!!!!!!!!!!!!\n",
    "            # INSERT YOUR CODE HERE!\n",
    "            # !!!!!!!!!!!!!!!!!!!!!!\n",
    "            y_pred = model(x)\n",
    "            loss, meta = criterion(y_pred, y, meta_agg='sum')\n",
    "            for k, v in meta.items():\n",
    "                val_meta[k] += v\n",
    "                \n",
    "        val_meta = {k: v / len(val_loader.dataset) for k, v in val_meta.items()}\n",
    "        val_loss = val_meta[\"loss\"]\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "    return val_loss.item(), val_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "707b273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(total_iter, opt, learning_rate, warmup_iters):\n",
    "    \"\"\"\n",
    "    For training we use one-cycle learning rate schedule\n",
    "    Firstly, linear growth of scale\n",
    "    Then, exponential decay\n",
    "    \"\"\"\n",
    "    if total_iter > warmup_iters:\n",
    "        scale = 1.0 / (total_iter ** 0.5)\n",
    "    else:\n",
    "        scale = total_iter / (warmup_iters ** 1.5)\n",
    "\n",
    "    for param_group in opt.param_groups:\n",
    "        param_group[\"lr\"] = learning_rate * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13b3b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fastpitch(dataset_dir, train_filelist_path, val_filelist_path, logdir, hparams):\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(hparams.seed)\n",
    "    torch.manual_seed(hparams.seed)\n",
    "    torch.cuda.manual_seed(hparams.seed)\n",
    "\n",
    "    model = FastPitch(hparams).to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = Lamb(\n",
    "        model.parameters(),\n",
    "        lr=hparams.learning_rate,\n",
    "        betas=(0.9, 0.98),\n",
    "        eps=1e-9,\n",
    "        weight_decay=hparams.weight_decay\n",
    "    )\n",
    "\n",
    "    logger = FastpitchLogger(logdir, hparams)\n",
    "    train_loader, valid_loader = prepare_loaders(dataset_dir, train_filelist_path, val_filelist_path, hparams)\n",
    "    criterion = FastPitchLoss(hparams)\n",
    "\n",
    "    checkpoint = FastpitchLogger.load_last_checkpoint(logdir)\n",
    "    step = checkpoint['step'] or 0\n",
    "    if checkpoint['state_dict']:\n",
    "        load_from_checkpoint(model, checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = step // len(train_loader)\n",
    "\n",
    "    while True:\n",
    "        if hasattr(train_loader.sampler, 'set_epoch'):\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "        if hasattr(train_loader.batch_sampler, 'set_epoch'):\n",
    "            train_loader.batch_sampler.set_epoch(epoch)\n",
    "\n",
    "        for _, batch in enumerate(train_loader):\n",
    "            x, y = batch_to_device(batch, device)\n",
    "            \n",
    "            adjust_learning_rate(step, optimizer, hparams.learning_rate, hparams.warmup_steps)\n",
    "            model.zero_grad()\n",
    "\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            # !!!!!!!!!!!!!!!!!!!!!!\n",
    "            # INSERT YOUR CODE HERE!\n",
    "            # !!!!!!!!!!!!!!!!!!!!!!\n",
    "            loss, meta = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            logger.log_grads(step, model)\n",
    "\n",
    "            # !!!!!!!!!!!!!!!!!!!!!!\n",
    "            # INSERT YOUR CODE HERE!\n",
    "            # !!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "            step += 1\n",
    "    \n",
    "            meta['step'] = step\n",
    "            meta['lr'] = optimizer.param_groups[0][\"lr\"]\n",
    "    \n",
    "            if True or step % hparams.eval_interval == 0:\n",
    "                loss, meta = validate(model, criterion, valid_loader, device, use_gt_durations=True)\n",
    "\n",
    "                meta['step'] = step\n",
    "                logger.log_training(step, meta)\n",
    "                logger.log_validation(step, meta)\n",
    "                logger.save_checkpoint(model, optimizer, step, hparams)\n",
    "    \n",
    "            if step >= hparams.num_steps:\n",
    "                return\n",
    "            break\n",
    "\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da1b291",
   "metadata": {},
   "source": [
    "There are some signs of convergence after the first day of training at 1 GPU.\n",
    "\n",
    "After ~3 days the model is fully trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27b6d1a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 1  loss 36.4579\n",
      "val : 1  loss 36.457908630371094\n",
      "train : 2  loss 36.4579\n",
      "val : 2  loss 36.457908630371094\n",
      "train : 3  loss 36.4579\n",
      "val : 3  loss 36.457908630371094\n",
      "train : 4  loss 36.4579\n",
      "val : 4  loss 36.457908630371094\n",
      "train : 5  loss 36.4579\n",
      "val : 5  loss 36.457908630371094\n",
      "train : 6  loss 36.4579\n",
      "val : 6  loss 36.457908630371094\n",
      "train : 7  loss 36.4579\n",
      "val : 7  loss 36.457908630371094\n",
      "train : 8  loss 36.4579\n",
      "val : 8  loss 36.457908630371094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-59900ceb0d67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_fastpitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepr_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_filelist_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_filelist_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-e766504f2f08>\u001b[0m in \u001b[0;36mtrain_fastpitch\u001b[0;34m(dataset_dir, train_filelist_path, val_filelist_path, logdir, hparams)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gt_durations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-b30ef15c157c>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, criterion, val_loader, device, use_gt_durations)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mval_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# !!!!!!!!!!!!!!!!!!!!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-eb990923c9a5>\u001b[0m in \u001b[0;36mbatch_to_device\u001b[0;34m(batch, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                for tensor in batch]\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-eb990923c9a5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                for tensor in batch]\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_fastpitch(prepr_data_path, train_filelist_path, val_filelist_path, logs_dir, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8653a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs_dir/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e387caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45cb444c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 18433), started 0:01:11 ago. (Use '!kill 18433' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-baf934b9d7724e54\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-baf934b9d7724e54\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in dir(hparams):\n",
    "    print(attr, getattr(hparams, attr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90984e7c",
   "metadata": {},
   "source": [
    "# GST Space\n",
    "\n",
    "Here you should visualize the latent GST space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01933ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Please run the model GST encoder over the whole trainset or subcorpus and visualize\n",
    "\"\"\"\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# INSERT YOUR CODE HERE!\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "GSTVec = np.stack(GSTStyles)\n",
    "GSTVec = GSTVec.reshape(GSTVec.shape[0], -1)\n",
    "GSTEmbedded = TSNE(n_components=2).fit_transform(GSTVec)\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "# INSERT YOUR CODE HERE!\n",
    "# !!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\"\"\"\n",
    "Visualize space and try to listen to examples of some clusters you've noticed in the map\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e941ee2",
   "metadata": {},
   "source": [
    "# GST estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleEmbeddingEstimator(nn.Module):\n",
    "    \"\"\"\n",
    "    Estimator takes encoder outputs and returns style embeddings\n",
    "    You can use some aggregation layers here: GRU or FFT + FC and some dropout for regularization\n",
    "    Output dimension should be hparams.symbols_embedding_dim\n",
    "    \"\"\"\n",
    "    def __init__(self, hparams):\n",
    "        super(StyleEmbeddingEstimator, self).__init__()\n",
    "    \n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b872d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gst_estimator_checkpoint(predictor, optimizer, filepath):\n",
    "    checkpoint = torch.load(filepath, map_location=\"cpu\")\n",
    "    predictor.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    return checkpoint['step'], checkpoint['min_eval_loss']\n",
    "\n",
    "\n",
    "def save_gst_estimator_checkpoint(filepath, step, min_eval_loss, model, optimizer):\n",
    "    checkpoint = {\n",
    "        \"step\": step,\n",
    "        'min_eval_loss': min_eval_loss,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gst_estimator(dataset_dir, train_filelist_path, val_filelist_path, \n",
    "          fastpitch_checkpoint_path, logdir):\n",
    "    fastpitch_checkpoint = torch.load(fastpitch_checkpoint_path)\n",
    "    hparams = HParamsFastpitch(fastpitch_checkpoint['params'])\n",
    "    fastpitch = FastPitch(hparams).to(device)\n",
    "    load_from_checkpoint(fastpitch, fastpitch_checkpoint['state_dict'])\n",
    "    fastpitch.eval()\n",
    "    \n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(hparams.seed)\n",
    "    torch.manual_seed(hparams.seed)\n",
    "    torch.cuda.manual_seed(hparams.seed)\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "    train_writer = SummaryWriter(os.path.join(logdir, \"train\"))\n",
    "    val_writer = SummaryWriter(os.path.join(logdir, \"val\"))\n",
    "    \n",
    "    gst_estimator = StyleEmbeddingEstimator(hparams).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(gst_estimator.parameters(), lr=hparams.gst_estimator_lr)\n",
    "    \n",
    "    train_loader, eval_loader = prepare_loaders(dataset_dir, train_filelist_path, val_filelist_path, hparams)\n",
    "    \n",
    "    step = 0\n",
    "    min_eval_loss = np.inf\n",
    "\n",
    "    checkpoint_path = os.path.join(logdir, f\"GST_estimator_best_checkpoint.pt\")\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        print(\"Resume training from checkpoint: \", checkpoint_path)\n",
    "        step, min_eval_loss = load_gst_estimator_checkpoint(gst_estimator, optimizer, checkpoint_path)\n",
    "\n",
    "    losses = []\n",
    "    gst_estimator.train()\n",
    "    while True:\n",
    "        for batch in train_loader:\n",
    "            x, y = batch_to_device(batch, device)\n",
    "            \"\"\"\n",
    "            First, get the GT values for styles and encoder_state:\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                gst_true = ...\n",
    "                enc_out = ... \n",
    "                \n",
    "            # !!!!!!!!!!!!!!!!!!!!!!\n",
    "            # INSERT YOUR CODE HERE!\n",
    "            # !!!!!!!!!!!!!!!!!!!!!!\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            gst_pred = ...\n",
    "            \n",
    "            \"\"\"\n",
    "            It is better to train GST estimator as l1 regression\n",
    "            \"\"\"\n",
    "            loss = ...\n",
    "            \n",
    "            # !!!!!!!!!!!!!!!!!!!!!!\n",
    "            # INSERT YOUR CODE HERE!\n",
    "            # !!!!!!!!!!!!!!!!!!!!!!\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            step += 1\n",
    "            \n",
    "            if step % hparams.gst_estimator_eval_interval == 0:\n",
    "                train_writer.add_scalar('loss', np.mean(losses), step)\n",
    "                print(f\"train: {step:<3d} loss: {np.mean(losses):<5.4f}\")\n",
    "                \n",
    "                losses = []\n",
    "                gst_estimator.eval()\n",
    "                for batch in eval_loader:\n",
    "                    x, y = batch_to_device(batch, device)\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    The same, but for validation:\n",
    "                    \"\"\"\n",
    "                     \n",
    "                    with torch.no_grad():\n",
    "                        gst_true = ...\n",
    "                        enc_out = ...\n",
    "                        gst_pred = ...\n",
    "                        loss = ...\n",
    "                        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "                        # INSERT YOUR CODE HERE!\n",
    "                        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "                        losses.append(loss.item())\n",
    "                         \n",
    "                val_writer.add_scalar('loss', np.mean(losses), step)\n",
    "                print(f\"val: {step:<3d} loss: {np.mean(losses):<5.4f}\")\n",
    "                \n",
    "                \"\"\"\n",
    "                Fallback to the prev model if the new one is not better:\n",
    "                \"\"\"\n",
    "                if np.mean(losses) < min_eval_loss:\n",
    "                    min_eval_loss = np.mean(losses)\n",
    "                    checkpoint_path = os.path.join(logdir, f\"GST_estimator_best_checkpoint.pt\")\n",
    "                    save_gst_estimator_checkpoint(checkpoint_path, step, min_eval_loss, gst_estimator, optimizer)\n",
    "                                \n",
    "                for w in train_writer.all_writers.values():\n",
    "                    w.flush()\n",
    "                for w in val_writer.all_writers.values():\n",
    "                    w.flush()\n",
    "\n",
    "                if step >= hparams.gst_estimator_num_steps:\n",
    "                    exit()\n",
    "                \n",
    "                losses = []\n",
    "                gst_estimator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f87e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fastpitch_checkpoint_path = \"logs_dir/0000200_snapshot.pth\"\n",
    "\n",
    "\"\"\"\n",
    "Usually few hours is enough to train the model well\n",
    "\"\"\"\n",
    "\n",
    "train_gst_estimator(prepr_data_path, train_filelist_path, val_filelist_path, \n",
    "          fastpitch_checkpoint_path, gst_logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8672b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gst_estimator_checkpoint_path = os.path.join(gst_logs_dir, \"GST_estimator_best_checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac0ef8d",
   "metadata": {},
   "source": [
    "# Full model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76b03f",
   "metadata": {},
   "source": [
    "## load waveglow checkpoint\n",
    "\n",
    "here we are using the same vocoder -- waveglow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_amp_256/versions/20.01.0/zip\"\n",
    "wg_checkpoint_path = os.path.join(pretrained_models_path, \"waveglow\")\n",
    "wg_zip_path = os.path.join(pretrained_models_path, \"waveglow.zip\")\n",
    "wg_unpacked_path = os.path.join(pretrained_models_path, 'nvidia_waveglow256pyt_fp16.pt')\n",
    "\n",
    "\n",
    "getfile(wg_url, wg_zip_path)\n",
    "shutil.unpack_archive(wg_zip_path, pretrained_models_path)\n",
    "shutil.move(wg_unpacked_path, wg_checkpoint_path)\n",
    "os.remove(wg_zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a65d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(wg_checkpoint_path, fastpitch_checkpoint_path, gst_estimator_checkpoint_path,\n",
    "              test_texts, synth_wavs_path):\n",
    "    wg_checkpoint = torch.load(wg_checkpoint_path)\n",
    "    waveglow = WaveGlow(**wg_checkpoint['config']).to(device)\n",
    "    load_from_checkpoint(waveglow, wg_checkpoint['state_dict'])\n",
    "    waveglow = waveglow.remove_weightnorm(waveglow)\n",
    "    waveglow.eval()\n",
    "    \n",
    "    denoiser = Denoiser(waveglow).to(device)\n",
    "    \n",
    "    fastpitch_checkpoint = torch.load(fastpitch_checkpoint_path)\n",
    "    hparams = HParamsFastpitch(fastpitch_checkpoint['params'])\n",
    "    \n",
    "    fastpitch = FastPitch(hparams).to(device)\n",
    "    load_from_checkpoint(fastpitch, fastpitch_checkpoint['state_dict'])\n",
    "    fastpitch.eval()\n",
    "    \n",
    "    gst_estimator_checkpoint = torch.load(gst_estimator_checkpoint_path)\n",
    "    gst_estimator = StyleEmbeddingEstimator(hparams).to(device)\n",
    "    load_from_checkpoint(gst_estimator, gst_estimator_checkpoint['state_dict'])\n",
    "    gst_estimator.eval()\n",
    "    \n",
    "    texts_encoded = [(fid, tp.encode_text(txt)) for fid, txt in test_texts.items()]\n",
    "\n",
    "    if not os.path.exists(synth_wavs_path):\n",
    "        os.mkdir(synth_wavs_path)\n",
    "    \n",
    "    for fid, text in tqdm.tqdm(texts_encoded):\n",
    "        with torch.no_grad():\n",
    "            text = torch.LongTensor(text)\n",
    "            text_length = torch.LongTensor([text.size(0)]).to(device)\n",
    "            text = text.unsqueeze(0).to(device)\n",
    "            mel_pred = fastpitch.infer((text, text_length), gst_estimator)\n",
    "            audio_pred = waveglow.infer(mel_pred, sigma=hparams.wg_sigma_infer)\n",
    "            audio_pred = denoiser(audio_pred.float(), strength=hparams.wg_denoising_strength)\n",
    "            audio_pred = audio_pred.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "            wavfile.write(os.path.join(synth_wavs_path, fid + \".wav\"), hparams.sample_rate, \n",
    "                          (np.clip(audio_pred, -1, 1) * 32767).astype(np.int16))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903168ff",
   "metadata": {},
   "source": [
    "#### Check yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b3f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(wg_checkpoint_path, fastpitch_checkpoint_path, gst_estimator_checkpoint_path, \n",
    "          {\"TST\": \"The quick brown fox jumps over the lazy dog.\"}, synth_wavs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a34cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.display(ipd.Audio('synth_wavs/TST.wav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ec5c0",
   "metadata": {},
   "source": [
    "# The test\n",
    "Here is the estimation of your model:\n",
    "\n",
    "you should render records from the given file 'test_texts.txt' ,\n",
    "they should be intelligible and correspond to the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_texts.txt') as ifile:\n",
    "    test_data = {\n",
    "        \"S{:03}_test\".format(i): l.strip()\n",
    "        for i, l in enumerate(ifile)\n",
    "    }\n",
    "    \n",
    "sorted(test_data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(wg_checkpoint_path, fastpitch_checkpoint_path, gst_estimator_checkpoint_path, \n",
    "          test_data, synth_wavs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d8e461",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for fid, text in sorted(test_data.items()):\n",
    "    print(fid)\n",
    "    print(text)\n",
    "    wav_filename = os.path.join(synth_wavs_path, fid + \".wav\")\n",
    "    ipd.display(ipd.Audio(wav_filename))\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3453e0",
   "metadata": {},
   "source": [
    "## GST\n",
    "\n",
    "the style token\n",
    "\n",
    "It is recommended for you to use already implemented FFT block as a GST encoder.\n",
    "\n",
    "For the details - look at the lecture and/or paper https://arxiv.org/pdf/1803.09017.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ae5acec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass ReferenceEncoder(nn.Module):\\n    '''\\n    inputs --- [N, Ty/r, n_mels*r]  mels\\n    outputs --- [N, ref_enc_gru_size]\\n    '''\\n\\n    def __init__(self, hparams):\\n\\n        super().__init__()\\n        ref_enc_filters = [32, 32, 64, 64, 128, 128]\\n        ref_enc_size = [3, 3]\\n        ref_enc_strides = [2, 2]\\n        K = len(ref_enc_filters)\\n        filters = [1] + ref_enc_filters\\n        convs = [nn.Conv2d(in_channels=filters[i],\\n                           out_channels=filters[i + 1],\\n                           kernel_size=(3, 3),\\n                           stride=(2, 2),\\n                           padding=(1, 1)) for i in range(K)]\\n        self.convs = nn.ModuleList(convs)\\n        self.bns = nn.ModuleList([nn.BatchNorm2d(num_features=ref_enc_filters[i]) for i in range(K)])\\n\\n        out_channels = self.calculate_channels(hparams.n_mel_channels, 3, 2, 1, K)\\n        self.gru = nn.GRU(input_size=ref_enc_filters[-1] * out_channels,\\n                          hidden_size=hparams.symbols_embedding_dim // 2,\\n                          batch_first=True)\\n        self.n_mel_channels = hparams.n_mel_channels\\n\\n    def forward(self, inputs):\\n        N = inputs.size(0)\\n        out = inputs.view(N, 1, -1, self.n_mel_channels)  # [N, 1, Ty, n_mels]\\n        for conv, bn in zip(self.convs, self.bns):\\n            out = conv(out)\\n            out = bn(out)\\n            out = F.relu(out)  # [N, 128, Ty//2^K, n_mels//2^K]\\n\\n        out = out.transpose(1, 2)  # [N, Ty//2^K, 128, n_mels//2^K]\\n        T = out.size(1)\\n        N = out.size(0)\\n        out = out.contiguous().view(N, T, -1)  # [N, Ty//2^K, 128*n_mels//2^K]\\n\\n        self.gru.flatten_parameters()\\n        memory, out = self.gru(out)  # out --- [1, N, E//2]\\n\\n        return out.squeeze(0)\\n\\n    def calculate_channels(self, L, kernel_size, stride, pad, n_convs):\\n        for i in range(n_convs):\\n            L = (L - kernel_size + 2 * pad) // stride + 1\\n        return L\\n\\n\\nclass MultiHeadAttention(nn.Module):\\n    '''\\n    input:\\n        query --- [N, T_q, query_dim]\\n        key --- [N, T_k, key_dim]\\n    output:\\n        out --- [N, T_q, num_units]\\n    '''\\n\\n    def __init__(self, query_dim, key_dim, num_units, num_heads):\\n\\n        super().__init__()\\n        self.num_units = num_units\\n        self.num_heads = num_heads\\n        self.key_dim = key_dim\\n\\n        self.W_query = nn.Linear(in_features=query_dim, out_features=num_units, bias=False)\\n        self.W_key = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\\n        self.W_value = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\\n\\n    def forward(self, query, key):\\n        querys = self.W_query(query)  # [N, T_q, num_units]\\n        keys = self.W_key(key)  # [N, T_k, num_units]\\n        values = self.W_value(key)\\n\\n        split_size = self.num_units // self.num_heads\\n        querys = torch.stack(torch.split(querys, split_size, dim=2), dim=0)  # [h, N, T_q, num_units/h]\\n        keys = torch.stack(torch.split(keys, split_size, dim=2), dim=0)  # [h, N, T_k, num_units/h]\\n        values = torch.stack(torch.split(values, split_size, dim=2), dim=0)  # [h, N, T_k, num_units/h]\\n\\n        # score = softmax(QK^T / (d_k ** 0.5))\\n        scores = torch.matmul(querys, keys.transpose(2, 3))  # [h, N, T_q, T_k]\\n        scores = scores / (self.key_dim ** 0.5)\\n        scores = F.softmax(scores, dim=3)\\n\\n        # out = score * V\\n        out = torch.matmul(scores, values)  # [h, N, T_q, num_units/h]\\n        out = torch.cat(torch.split(out, 1, dim=0), dim=3).squeeze(0)  # [N, T_q, num_units]\\n\\n        return out\\n    \""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class ReferenceEncoder(nn.Module):\n",
    "    '''\n",
    "    inputs --- [N, Ty/r, n_mels*r]  mels\n",
    "    outputs --- [N, ref_enc_gru_size]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "\n",
    "        super().__init__()\n",
    "        ref_enc_filters = [32, 32, 64, 64, 128, 128]\n",
    "        ref_enc_size = [3, 3]\n",
    "        ref_enc_strides = [2, 2]\n",
    "        K = len(ref_enc_filters)\n",
    "        filters = [1] + ref_enc_filters\n",
    "        convs = [nn.Conv2d(in_channels=filters[i],\n",
    "                           out_channels=filters[i + 1],\n",
    "                           kernel_size=(3, 3),\n",
    "                           stride=(2, 2),\n",
    "                           padding=(1, 1)) for i in range(K)]\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm2d(num_features=ref_enc_filters[i]) for i in range(K)])\n",
    "\n",
    "        out_channels = self.calculate_channels(hparams.n_mel_channels, 3, 2, 1, K)\n",
    "        self.gru = nn.GRU(input_size=ref_enc_filters[-1] * out_channels,\n",
    "                          hidden_size=hparams.symbols_embedding_dim // 2,\n",
    "                          batch_first=True)\n",
    "        self.n_mel_channels = hparams.n_mel_channels\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        N = inputs.size(0)\n",
    "        out = inputs.view(N, 1, -1, self.n_mel_channels)  # [N, 1, Ty, n_mels]\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            out = conv(out)\n",
    "            out = bn(out)\n",
    "            out = F.relu(out)  # [N, 128, Ty//2^K, n_mels//2^K]\n",
    "\n",
    "        out = out.transpose(1, 2)  # [N, Ty//2^K, 128, n_mels//2^K]\n",
    "        T = out.size(1)\n",
    "        N = out.size(0)\n",
    "        out = out.contiguous().view(N, T, -1)  # [N, Ty//2^K, 128*n_mels//2^K]\n",
    "\n",
    "        self.gru.flatten_parameters()\n",
    "        memory, out = self.gru(out)  # out --- [1, N, E//2]\n",
    "\n",
    "        return out.squeeze(0)\n",
    "\n",
    "    def calculate_channels(self, L, kernel_size, stride, pad, n_convs):\n",
    "        for i in range(n_convs):\n",
    "            L = (L - kernel_size + 2 * pad) // stride + 1\n",
    "        return L\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    input:\n",
    "        query --- [N, T_q, query_dim]\n",
    "        key --- [N, T_k, key_dim]\n",
    "    output:\n",
    "        out --- [N, T_q, num_units]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, query_dim, key_dim, num_units, num_heads):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_units = num_units\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "\n",
    "        self.W_query = nn.Linear(in_features=query_dim, out_features=num_units, bias=False)\n",
    "        self.W_key = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n",
    "        self.W_value = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n",
    "\n",
    "    def forward(self, query, key):\n",
    "        querys = self.W_query(query)  # [N, T_q, num_units]\n",
    "        keys = self.W_key(key)  # [N, T_k, num_units]\n",
    "        values = self.W_value(key)\n",
    "\n",
    "        split_size = self.num_units // self.num_heads\n",
    "        querys = torch.stack(torch.split(querys, split_size, dim=2), dim=0)  # [h, N, T_q, num_units/h]\n",
    "        keys = torch.stack(torch.split(keys, split_size, dim=2), dim=0)  # [h, N, T_k, num_units/h]\n",
    "        values = torch.stack(torch.split(values, split_size, dim=2), dim=0)  # [h, N, T_k, num_units/h]\n",
    "\n",
    "        # score = softmax(QK^T / (d_k ** 0.5))\n",
    "        scores = torch.matmul(querys, keys.transpose(2, 3))  # [h, N, T_q, T_k]\n",
    "        scores = scores / (self.key_dim ** 0.5)\n",
    "        scores = F.softmax(scores, dim=3)\n",
    "\n",
    "        # out = score * V\n",
    "        out = torch.matmul(scores, values)  # [h, N, T_q, num_units/h]\n",
    "        out = torch.cat(torch.split(out, 1, dim=0), dim=3).squeeze(0)  # [N, T_q, num_units]\n",
    "\n",
    "        return out\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afbc58b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass GST(nn.Module):\\n    def __init__(self, hparams):\\n        super(GST, self).__init__()\\n\\n        # additional layer to be used in GST\\n        # !!!!!!!!!!!!!!!!!!!!!!\\n        # INSERT YOUR CODE HERE!\\n        # !!!!!!!!!!!!!!!!!!!!!!\\n        self.encoder = ReferenceEncoder(hparams)\\n        d_q = hparams.gst_d_head // 2\\n        d_k = hparams.gst_d_head // hparams.gst_n_heads\\n        self.embed = nn.Parameter(torch.FloatTensor(d_k, hparams.symbols_embedding_dim // hparams.gst_n_heads))\\n        self.attention = MultiHeadAttention(query_dim=hparams.symbols_embedding_dim // 2,\\n                                            key_dim=hparams.symbols_embedding_dim // hparams.gst_n_heads,\\n                                            num_units=hparams.symbols_embedding_dim,\\n                                            num_heads=hparams.gst_n_heads)\\n        # MultiHeadAttn(hparams.gst_n_heads, hparams.symbols_embedding_dim, hparams.gst_d_head, hparams.p_gst_dropout, hparams.p_gst_dropatt)\\n\\n        # init.normal_(self.embed, mean=0, std=0.5)\\n\\n    def forward(self, mels, mel_lengths):\\n        \"\"\"\\n        Should take GT mels and return style embeddings and attention probs\\n        \"\"\"\\n        mels_enc = self.encoder(mels)\\n        N = mels_enc.size(0)\\n        query = mels_enc.unsqueeze(1)  # [N, 1, E//2]\\n        keys = F.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)  # [N, token_num, E // num_heads]\\n        style_embed = self.attention(query, keys)\\n        \\n        \"\"\"\\n        mel-invariant tokens should be keys in the attention here\\n        and encoder output -- the query\\n        \\n        \\n        Please note that you should prevent unlimited growth of tokens magnitude somehow\\n        E.g. you can shrink them with some restrictive function:\\n            attention_keys = tanh(tokens_embeddings)\\n        \"\"\"\\n    \\n        # !!!!!!!!!!!!!!!!!!!!!!\\n        # INSERT YOUR CODE HERE!\\n        # !!!!!!!!!!!!!!!!!!!!!!\\n        return style_embed\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class GST(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super(GST, self).__init__()\n",
    "\n",
    "        # additional layer to be used in GST\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        self.encoder = ReferenceEncoder(hparams)\n",
    "        d_q = hparams.gst_d_head // 2\n",
    "        d_k = hparams.gst_d_head // hparams.gst_n_heads\n",
    "        self.embed = nn.Parameter(torch.FloatTensor(d_k, hparams.symbols_embedding_dim // hparams.gst_n_heads))\n",
    "        self.attention = MultiHeadAttention(query_dim=hparams.symbols_embedding_dim // 2,\n",
    "                                            key_dim=hparams.symbols_embedding_dim // hparams.gst_n_heads,\n",
    "                                            num_units=hparams.symbols_embedding_dim,\n",
    "                                            num_heads=hparams.gst_n_heads)\n",
    "        # MultiHeadAttn(hparams.gst_n_heads, hparams.symbols_embedding_dim, hparams.gst_d_head, hparams.p_gst_dropout, hparams.p_gst_dropatt)\n",
    "\n",
    "        # init.normal_(self.embed, mean=0, std=0.5)\n",
    "\n",
    "    def forward(self, mels, mel_lengths):\n",
    "        \"\"\"\n",
    "        Should take GT mels and return style embeddings and attention probs\n",
    "        \"\"\"\n",
    "        mels_enc = self.encoder(mels)\n",
    "        N = mels_enc.size(0)\n",
    "        query = mels_enc.unsqueeze(1)  # [N, 1, E//2]\n",
    "        keys = F.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)  # [N, token_num, E // num_heads]\n",
    "        style_embed = self.attention(query, keys)\n",
    "        \n",
    "        \"\"\"\n",
    "        mel-invariant tokens should be keys in the attention here\n",
    "        and encoder output -- the query\n",
    "        \n",
    "        \n",
    "        Please note that you should prevent unlimited growth of tokens magnitude somehow\n",
    "        E.g. you can shrink them with some restrictive function:\n",
    "            attention_keys = tanh(tokens_embeddings)\n",
    "        \"\"\"\n",
    "    \n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        # INSERT YOUR CODE HERE!\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!\n",
    "        return style_embed\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
